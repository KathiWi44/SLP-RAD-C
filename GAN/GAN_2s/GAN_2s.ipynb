{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad27b7c5-de56-4881-af47-53f5bd114b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 471 audio files.\n",
      "Mel-spectrogram shape for file 0: (64, 87)\n",
      "Data shape after converting to np.array: (471, 64, 87)\n",
      "Data shape after adding channel dimension: (471, 64, 87, 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf  # For saving audio files\n",
    "import librosa.display\n",
    "\n",
    "# Path to the main folder\n",
    "base_dir = r\"C:\\Users\\X\\PROCESS-V1\"\n",
    "\n",
    "# Preprocessing parameters\n",
    "SR = 22050       # Sample rate\n",
    "DURATION = 2.0   # Duration in seconds\n",
    "N_MELS = 64      # Number of mel bands\n",
    "HOP_LENGTH = 512 # Hop length for STFT\n",
    "FIXED_LENGTH = int(SR * DURATION)  # Number of samples for fixed duration\n",
    "\n",
    "# Function to load all .wav files recursively\n",
    "def load_audio_files(base_dir):\n",
    "    audio_files = []\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "    return audio_files\n",
    "\n",
    "# Function to preprocess audio files\n",
    "def preprocess_audio(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=SR)\n",
    "    # Ensure audio is of fixed length\n",
    "    if len(y) < FIXED_LENGTH:\n",
    "        # Pad audio\n",
    "        padding = FIXED_LENGTH - len(y)\n",
    "        y = np.pad(y, (0, padding), 'constant')\n",
    "    else:\n",
    "        y = y[:FIXED_LENGTH]\n",
    "    # Compute mel-spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=N_MELS, hop_length=HOP_LENGTH)\n",
    "    # Convert to log scale (dB)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return mel_spec_db\n",
    "\n",
    "# Load all .wav file paths\n",
    "audio_file_paths = load_audio_files(base_dir)\n",
    "print(f\"Found {len(audio_file_paths)} audio files.\")\n",
    "\n",
    "# Preprocess all audio files\n",
    "data = []\n",
    "for idx, file_path in enumerate(audio_file_paths):\n",
    "    mel_spec = preprocess_audio(file_path)\n",
    "    if idx == 0:\n",
    "        print(f\"Mel-spectrogram shape for file {idx}: {mel_spec.shape}\")\n",
    "    data.append(mel_spec)\n",
    "\n",
    "data = np.array(data)\n",
    "print(f\"Data shape after converting to np.array: {data.shape}\")  # Should be (num_samples, N_MELS, time_steps)\n",
    "\n",
    "# Normalize data to [-1, 1]\n",
    "data_min = data.min()\n",
    "data_max = data.max()\n",
    "data = (data - data_min) / (data_max - data_min) * 2 - 1\n",
    "\n",
    "# Add a channel dimension\n",
    "data = data[..., np.newaxis]\n",
    "print(f\"Data shape after adding channel dimension: {data.shape}\")  # Should be (num_samples, N_MELS, time_steps, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "160fd75b-3c48-4497-a7c2-636f4e7a2e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n",
      "3.6.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)\n",
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6dc104f2-d547-4a75-9022-3706e5833341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Conv2D, Conv2DTranspose, Flatten, Reshape,\n",
    "    LeakyReLU, BatchNormalization, ZeroPadding2D, Cropping2D\n",
    ")\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "# Input shape parameters\n",
    "num_samples, N_MELS, time_steps, channels = data.shape\n",
    "input_shape = (N_MELS, time_steps, channels)\n",
    "latent_dim = 100  # Size of the latent space (noise vector)\n",
    "\n",
    "def build_generator(latent_dim):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Starting dimensions\n",
    "    n_rows, n_cols = 8, 11  # Choose values so that upsampling results in (64, 87)\n",
    "    model.add(Dense(256 * n_rows * n_cols, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((n_rows, n_cols, 256)))  # Shape: (8, 11, 256)\n",
    "    \n",
    "    # First upsampling\n",
    "    model.add(Conv2DTranspose(256, kernel_size=4, strides=(2, 2), padding='same'))  # Shape: (16, 22, 256)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    # Second upsampling\n",
    "    model.add(Conv2DTranspose(128, kernel_size=4, strides=(2, 2), padding='same'))  # Shape: (32, 44, 128)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    # Third upsampling\n",
    "    model.add(Conv2DTranspose(64, kernel_size=4, strides=(2, 2), padding='same'))  # Shape: (64, 88, 64)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    # Final adjustment to reach (64, 87, 1)\n",
    "    model.add(Conv2DTranspose(1, kernel_size=4, strides=(1, 1), padding='valid', activation='tanh'))  # Shape: (67, 91, 1)\n",
    "    # Adjusted cropping to achieve the desired output shape\n",
    "    model.add(Cropping2D(cropping=((1, 2), (2, 2))))  # Crop to (64, 87, 1)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_discriminator(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=4, strides=2, padding='same', input_shape=input_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Conv2D(256, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Build the generator and discriminator\n",
    "generator = build_generator(latent_dim)\n",
    "discriminator = build_discriminator(input_shape)\n",
    "\n",
    "# Compile the discriminator\n",
    "discriminator.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5),\n",
    "    run_eagerly=True\n",
    ")\n",
    "\n",
    "# Build and compile the GAN model\n",
    "discriminator.trainable = True  # Freeze the discriminator's weights when training the generator\n",
    "gan_input = Input(shape=(latent_dim,))\n",
    "generated_image = generator(gan_input)\n",
    "gan_output = discriminator(generated_image)\n",
    "gan = Model(gan_input, gan_output)\n",
    "gan.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f2c7b8a-26ba-4efc-b9a8-a7495b0db7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator trainable variables: 16\n",
      "Discriminator trainable variables: 12\n",
      "1 [D loss: 1.6667] [G loss: 1.1627]\n",
      "100 [D loss: 1.1811] [G loss: 2.9893]\n",
      "200 [D loss: 0.7966] [G loss: 1.8583]\n",
      "300 [D loss: 0.6356] [G loss: 2.1779]\n",
      "400 [D loss: 0.7577] [G loss: 1.6017]\n",
      "500 [D loss: 0.6798] [G loss: 2.4191]\n",
      "600 [D loss: 0.5619] [G loss: 1.4619]\n",
      "700 [D loss: 0.8638] [G loss: 2.5374]\n",
      "800 [D loss: 0.8812] [G loss: 1.7094]\n",
      "900 [D loss: 1.1074] [G loss: 2.6925]\n",
      "1000 [D loss: 0.7429] [G loss: 1.9955]\n",
      "1100 [D loss: 0.8201] [G loss: 1.6514]\n",
      "1200 [D loss: 1.2300] [G loss: 3.4045]\n",
      "1300 [D loss: 0.8119] [G loss: 1.3005]\n",
      "1400 [D loss: 0.6713] [G loss: 2.3096]\n",
      "1500 [D loss: 0.5316] [G loss: 2.0926]\n",
      "1600 [D loss: 0.6029] [G loss: 1.7156]\n",
      "1700 [D loss: 0.5676] [G loss: 2.1450]\n",
      "1800 [D loss: 0.7587] [G loss: 2.6285]\n",
      "1900 [D loss: 0.6179] [G loss: 2.3598]\n",
      "2000 [D loss: 0.6798] [G loss: 2.2865]\n",
      "2100 [D loss: 0.6010] [G loss: 3.6794]\n",
      "2200 [D loss: 0.5747] [G loss: 2.0159]\n",
      "2300 [D loss: 0.7191] [G loss: 3.2834]\n",
      "2400 [D loss: 0.6957] [G loss: 3.1874]\n",
      "2500 [D loss: 0.5678] [G loss: 2.3807]\n",
      "2600 [D loss: 0.4590] [G loss: 2.8835]\n",
      "2700 [D loss: 0.3728] [G loss: 2.8885]\n",
      "2800 [D loss: 0.3305] [G loss: 2.5905]\n",
      "2900 [D loss: 0.5176] [G loss: 3.9613]\n",
      "3000 [D loss: 0.4202] [G loss: 2.6063]\n",
      "3100 [D loss: 0.3410] [G loss: 2.7679]\n",
      "3200 [D loss: 0.3827] [G loss: 2.6651]\n",
      "3300 [D loss: 0.3261] [G loss: 2.8499]\n",
      "3400 [D loss: 0.2451] [G loss: 3.0690]\n",
      "3500 [D loss: 0.5963] [G loss: 2.9825]\n",
      "3600 [D loss: 0.3421] [G loss: 3.6131]\n",
      "3700 [D loss: 0.1092] [G loss: 3.5104]\n",
      "3800 [D loss: 0.2852] [G loss: 3.0506]\n",
      "3900 [D loss: 0.2156] [G loss: 3.6538]\n",
      "4000 [D loss: 0.1928] [G loss: 3.4924]\n",
      "4100 [D loss: 0.6830] [G loss: 5.9596]\n",
      "4200 [D loss: 0.1566] [G loss: 3.7157]\n",
      "4300 [D loss: 0.0983] [G loss: 3.9457]\n",
      "4400 [D loss: 0.6266] [G loss: 4.4308]\n",
      "4500 [D loss: 0.1643] [G loss: 3.2422]\n",
      "4600 [D loss: 0.1876] [G loss: 3.9983]\n",
      "4700 [D loss: 0.2265] [G loss: 4.6273]\n",
      "4800 [D loss: 0.1718] [G loss: 2.9436]\n",
      "4900 [D loss: 0.1117] [G loss: 3.5476]\n",
      "5000 [D loss: 0.0633] [G loss: 3.5875]\n",
      "5100 [D loss: 0.1054] [G loss: 4.2252]\n",
      "5200 [D loss: 0.3909] [G loss: 4.3168]\n",
      "5300 [D loss: 0.1116] [G loss: 4.0522]\n",
      "5400 [D loss: 0.1610] [G loss: 4.2085]\n",
      "5500 [D loss: 0.1038] [G loss: 4.0663]\n",
      "5600 [D loss: 0.0597] [G loss: 4.2575]\n",
      "5700 [D loss: 0.0838] [G loss: 4.4842]\n",
      "5800 [D loss: 0.0615] [G loss: 4.1285]\n",
      "5900 [D loss: 0.1132] [G loss: 4.5441]\n",
      "6000 [D loss: 0.0442] [G loss: 4.6697]\n",
      "6100 [D loss: 0.0743] [G loss: 5.2279]\n",
      "6200 [D loss: 4.0557] [G loss: 5.4629]\n",
      "6300 [D loss: 0.4981] [G loss: 5.2645]\n",
      "6400 [D loss: 0.1235] [G loss: 4.6244]\n",
      "6500 [D loss: 0.1559] [G loss: 3.8009]\n",
      "6600 [D loss: 0.0800] [G loss: 3.9811]\n",
      "6700 [D loss: 0.0667] [G loss: 4.4711]\n",
      "6800 [D loss: 0.0774] [G loss: 4.4547]\n",
      "6900 [D loss: 0.0419] [G loss: 4.6699]\n",
      "7000 [D loss: 0.0487] [G loss: 4.7988]\n",
      "7100 [D loss: 0.0625] [G loss: 4.5383]\n",
      "7200 [D loss: 2.3760] [G loss: 1.1174]\n",
      "7300 [D loss: 0.7774] [G loss: 2.8030]\n",
      "7400 [D loss: 0.1919] [G loss: 3.6746]\n",
      "7500 [D loss: 0.1348] [G loss: 3.4171]\n",
      "7600 [D loss: 0.0860] [G loss: 3.9412]\n",
      "7700 [D loss: 0.0621] [G loss: 4.2192]\n",
      "7800 [D loss: 0.0457] [G loss: 4.6816]\n",
      "7900 [D loss: 1.4634] [G loss: 1.5535]\n",
      "8000 [D loss: 0.2037] [G loss: 4.0244]\n",
      "8100 [D loss: 0.0920] [G loss: 4.4542]\n",
      "8200 [D loss: 0.0729] [G loss: 4.1935]\n",
      "8300 [D loss: 0.0275] [G loss: 4.8847]\n",
      "8400 [D loss: 0.0358] [G loss: 4.5984]\n",
      "8500 [D loss: 0.0450] [G loss: 4.7838]\n",
      "8600 [D loss: 0.0277] [G loss: 4.8780]\n",
      "8700 [D loss: 0.5173] [G loss: 5.7694]\n",
      "8800 [D loss: 0.1012] [G loss: 3.9942]\n",
      "8900 [D loss: 0.0463] [G loss: 4.3901]\n",
      "9000 [D loss: 0.0431] [G loss: 4.9608]\n",
      "9100 [D loss: 0.0331] [G loss: 4.9854]\n",
      "9200 [D loss: 0.0274] [G loss: 5.1682]\n",
      "9300 [D loss: 0.0251] [G loss: 5.2705]\n",
      "9400 [D loss: 0.0614] [G loss: 5.1288]\n",
      "9500 [D loss: 0.0173] [G loss: 5.3995]\n",
      "9600 [D loss: 0.0246] [G loss: 5.3016]\n",
      "9700 [D loss: 0.0287] [G loss: 5.2181]\n",
      "9800 [D loss: 0.0180] [G loss: 5.7286]\n",
      "9900 [D loss: 0.0287] [G loss: 5.5746]\n",
      "10000 [D loss: 0.0168] [G loss: 5.6306]\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "epochs = 10000\n",
    "batch_size = 32\n",
    "save_interval = 1000  # Save generated samples every 1000 epochs\n",
    "\n",
    "# Labels for real and fake images (unused in custom loop but kept for completeness)\n",
    "real = np.ones((batch_size, 1), dtype=np.float32)\n",
    "fake = np.zeros((batch_size, 1), dtype=np.float32)\n",
    "\n",
    "# Define loss function\n",
    "binary_cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "# Define optimizers\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "# Build the generator and discriminator (ensure these functions are defined)\n",
    "generator = build_generator(latent_dim)\n",
    "discriminator = build_discriminator(input_shape)\n",
    "\n",
    "# Ensure models are trainable\n",
    "generator.trainable = True\n",
    "discriminator.trainable = True\n",
    "\n",
    "# Training step function\n",
    "@tf.function\n",
    "def train_step(real_images):\n",
    "    # Generate noise\n",
    "    noise = tf.random.normal([batch_size, latent_dim])\n",
    "\n",
    "    # Train the discriminator\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        # Generate fake images\n",
    "        generated_images = generator(noise, training=True)\n",
    "        \n",
    "        # Discriminator outputs\n",
    "        real_output = discriminator(real_images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        # Calculate discriminator loss\n",
    "        disc_loss_real = binary_cross_entropy(tf.ones_like(real_output), real_output)\n",
    "        disc_loss_fake = binary_cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "        disc_loss = disc_loss_real + disc_loss_fake\n",
    "\n",
    "    # Calculate discriminator gradients\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    # Apply discriminator gradients\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    # Train the generator\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        # Generate fake images\n",
    "        generated_images = generator(noise, training=True)\n",
    "        \n",
    "        # Discriminator output for generated images\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        # Calculate generator loss\n",
    "        gen_loss = binary_cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "    # Calculate generator gradients\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    \n",
    "    # Apply generator gradients\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "    return disc_loss, gen_loss\n",
    "\n",
    "# Check trainable variables\n",
    "print(f\"Generator trainable variables: {len(generator.trainable_variables)}\")\n",
    "print(f\"Discriminator trainable variables: {len(discriminator.trainable_variables)}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Select a random batch of real images\n",
    "    idx = np.random.randint(0, data.shape[0], batch_size)\n",
    "    real_imgs = data[idx]\n",
    "\n",
    "    # Perform a training step\n",
    "    d_loss, g_loss = train_step(real_imgs)\n",
    "\n",
    "    # Print progress\n",
    "    if epoch % 100 == 0 or epoch == 1:\n",
    "        print(f\"{epoch} [D loss: {d_loss.numpy():.4f}] [G loss: {g_loss.numpy():.4f}]\")\n",
    "\n",
    "    # If at save interval, save generated image samples\n",
    "    if epoch % save_interval == 0:\n",
    "        # Generate and save images\n",
    "        noise = tf.random.normal([1, latent_dim])\n",
    "        gen_img = generator(noise, training=False)\n",
    "        gen_img = gen_img.numpy().squeeze()\n",
    "\n",
    "        # Rescale back to original scale\n",
    "        gen_img = (gen_img + 1) / 2  # Scale from [-1, 1] to [0, 1]\n",
    "        gen_img = gen_img * (data_max - data_min) + data_min  # Rescale to original data range\n",
    "\n",
    "        # Save the generated image\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        librosa.display.specshow(gen_img, sr=SR, hop_length=HOP_LENGTH, x_axis='time', y_axis='mel')\n",
    "        plt.title(f\"Generated Mel Spectrogram at Epoch {epoch}\")\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"generated_image_epoch_{epoch}.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30a23502-dc31-43aa-8e14-915695561208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000197180D3740> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000197180D3740> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288ms/step\n",
      "Generated audio saved to generated_audio.wav\n"
     ]
    }
   ],
   "source": [
    "def generate_audio_from_mel(generator, latent_dim, filename):\n",
    "    # Generate mel-spectrogram\n",
    "    noise = np.random.normal(0, 1, (1, latent_dim))\n",
    "    gen_mel = generator.predict(noise)\n",
    "    gen_mel = gen_mel.squeeze()\n",
    "    \n",
    "    # Rescale the generated mel-spectrogram\n",
    "    gen_mel = (gen_mel + 1) / 2  # Scale from [-1, 1] to [0, 1]\n",
    "    gen_mel = gen_mel * (data_max - data_min) + data_min  # Rescale to original data range\n",
    "    \n",
    "    # Convert mel-spectrogram (in dB) to power\n",
    "    gen_mel = librosa.db_to_power(gen_mel)\n",
    "    \n",
    "    # Invert the mel-spectrogram to a waveform\n",
    "    y = librosa.feature.inverse.mel_to_audio(\n",
    "        gen_mel,\n",
    "        sr=SR,\n",
    "        n_fft=2048,\n",
    "        hop_length=HOP_LENGTH,\n",
    "        win_length=2048,\n",
    "        n_iter=60,\n",
    "        power=1.0\n",
    "    )\n",
    "    \n",
    "    # Save the audio using soundfilehttps://www.tensorflow.org/api_docs/python/tf/function\n",
    "    sf.write(filename, y, SR)\n",
    "    print(f\"Generated audio saved to {filename}\")\n",
    "    \n",
    "# Example usage\n",
    "generate_audio_from_mel(generator, latent_dim, 'generated_audio.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53e463f5-3df4-42a9-a31b-e410cfa2462d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save final models\n",
    "generator.save(\"final_generator.h5\")\n",
    "discriminator.save(\"final_discriminator.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f3ebaa6-b7ce-4006-87db-09766228b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final models\n",
    "generator.save(\"final_generator.keras\")\n",
    "discriminator.save(\"final_discriminator.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a8b6b94-afe0-42af-88e1-cbecea5c2a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_0.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_1.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_2.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_3.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_4.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_5.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_6.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_7.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_8.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_9.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_10.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_11.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_12.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_13.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_14.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_15.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_16.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_17.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_18.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_19.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_20.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_21.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_22.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_23.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_24.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_25.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_26.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_27.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_28.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_29.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_30.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_31.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_32.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_33.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_34.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_35.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_36.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_37.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_38.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_39.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_40.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_41.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_42.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_43.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_44.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_45.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_46.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_47.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_48.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_49.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_50.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_51.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_52.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_53.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_54.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_55.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_56.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_57.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_58.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_59.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_60.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_61.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_62.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_63.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_64.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_65.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_66.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_67.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_68.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_69.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_70.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_71.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_72.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_73.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_74.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_75.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_76.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_77.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_78.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_79.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_80.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_81.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_82.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_83.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_84.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_85.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_86.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_87.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_88.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_89.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_90.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_91.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_92.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_93.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_94.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_95.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_96.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_97.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_98.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Generated audio saved to augmented_data_fixed_20s\\synthetic_sample_99.wav\n"
     ]
    }
   ],
   "source": [
    "def generate_audio_from_mel_fixed(generator, latent_dim, filename, max_duration=20.0):\n",
    "    \"\"\"\n",
    "    Generate audio from a trained GAN model and save it with fixed settings.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import librosa\n",
    "    import soundfile as sf\n",
    "\n",
    "    # Generate mel-spectrogram\n",
    "    noise = np.random.normal(0, 1, (1, latent_dim))\n",
    "    gen_mel = generator.predict(noise)\n",
    "    gen_mel = gen_mel.squeeze()\n",
    "\n",
    "    # Rescale the generated mel-spectrogram\n",
    "    gen_mel = (gen_mel + 1) / 2  # Scale from [-1, 1] to [0, 1]\n",
    "    gen_mel = gen_mel * (data_max - data_min) + data_min  # Rescale to original data range\n",
    "\n",
    "    # Convert mel-spectrogram (in dB) to power\n",
    "    gen_mel = librosa.db_to_power(gen_mel)\n",
    "\n",
    "    # Invert the mel-spectrogram to a waveform\n",
    "    y = librosa.feature.inverse.mel_to_audio(\n",
    "        gen_mel,\n",
    "        sr=SR,\n",
    "        n_fft=2048,\n",
    "        hop_length=HOP_LENGTH,\n",
    "        win_length=2048,\n",
    "        n_iter=60,\n",
    "        power=1.0\n",
    "    )\n",
    "\n",
    "    # Adjust length to be slightly variable but within 20 seconds\n",
    "    max_samples = int(SR * max_duration)\n",
    "    y = librosa.util.fix_length(y, size=np.random.randint(max_samples - 1000, max_samples + 1000))\n",
    "\n",
    "    # Save the audio using soundfile\n",
    "    sf.write(filename, y, SR)\n",
    "    print(f\"Generated audio saved to {filename}\")\n",
    "\n",
    "def generate_batch_audio_fixed(generator, latent_dim, num_samples, folder_path, max_duration=20.0):\n",
    "    \"\"\"\n",
    "    Generate a batch of audio files and save them to a folder.\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    for i in range(num_samples):\n",
    "        filename = os.path.join(folder_path, f\"synthetic_sample_{i}.wav\")\n",
    "        generate_audio_from_mel_fixed(generator, latent_dim, filename, max_duration=max_duration)\n",
    "\n",
    "generate_batch_audio_fixed(generator, latent_dim, num_samples=100, folder_path=\"augmented_data_fixed_20s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ffb22c-554e-4e35-bad2-da462380f4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
