{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad27b7c5-de56-4881-af47-53f5bd114b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 471 audio files.\n",
      "Data shape after converting to np.array: (1413, 64, 63)\n",
      "Data shape after adding channel dimension: (1413, 64, 63, 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf  # For saving audio files\n",
    "import librosa.display\n",
    "\n",
    "# Path to the main folder\n",
    "base_dir = r\"C:\\Users\\Peter\\Documents\\Sync\\SyncDokumente\\FH\\MA\\3. Semester\\SLP24_exercises-1\\PStA\\PROCESS-V1\"\n",
    "\n",
    "# Preprocessing parameters\n",
    "SR = 16000       # Sample rate\n",
    "DURATION = 2.0   # Duration in seconds\n",
    "N_MELS = 64      # Number of mel bands\n",
    "HOP_LENGTH = 512 # Hop length for STFT\n",
    "FIXED_LENGTH = int(SR * DURATION)  # Number of samples for fixed duration\n",
    "\n",
    "# Function to load all .wav files recursively\n",
    "def load_audio_files(base_dir):\n",
    "    audio_files = []\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "    return audio_files\n",
    "\n",
    "# Function to preprocess audio files\n",
    "def preprocess_audio_multiple_segments(file_path, num_segments=3):\n",
    "    y, sr = librosa.load(file_path, sr=SR)\n",
    "    segments = []\n",
    "    \n",
    "    for _ in range(num_segments):\n",
    "        if len(y) < FIXED_LENGTH:\n",
    "            # Pad audio\n",
    "            padding = FIXED_LENGTH - len(y)\n",
    "            y = np.pad(y, (0, padding), 'constant')\n",
    "            segment = y\n",
    "        else:\n",
    "            # Randomly select a start point\n",
    "            max_offset = len(y) - FIXED_LENGTH\n",
    "            start = random.randint(0, max_offset)\n",
    "            segment = y[start:start + FIXED_LENGTH]\n",
    "\n",
    "        # Compute mel-spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(y=segment, sr=sr, n_mels=N_MELS, hop_length=HOP_LENGTH)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        segments.append(mel_spec_db)\n",
    "\n",
    "    return segments\n",
    "\n",
    "# Load all .wav file paths\n",
    "audio_file_paths = load_audio_files(base_dir)\n",
    "print(f\"Found {len(audio_file_paths)} audio files.\")\n",
    "\n",
    "# Preprocess all audio files\n",
    "data = []\n",
    "for idx, file_path in enumerate(audio_file_paths):\n",
    "    mel_specs = preprocess_audio_multiple_segments(file_path, num_segments=3)\n",
    "    for mel_spec in mel_specs:\n",
    "        data.append(mel_spec)\n",
    "\n",
    "data = np.array(data)\n",
    "print(f\"Data shape after converting to np.array: {data.shape}\")  # Should be (num_samples, N_MELS, time_steps)\n",
    "\n",
    "# Normalize data to [-1, 1]\n",
    "data_min = data.min()\n",
    "data_max = data.max()\n",
    "data = (data - data_min) / (data_max - data_min) * 2 - 1\n",
    "\n",
    "# Add a channel dimension\n",
    "data = data[..., np.newaxis]\n",
    "print(f\"Data shape after adding channel dimension: {data.shape}\")  # Should be (num_samples, N_MELS, time_steps, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b383c619-73e3-4cb3-9e04-8005957406da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sampling rates in dataset: {16000}\n",
      "16000\n"
     ]
    }
   ],
   "source": [
    "def check_sampling_rates(file_paths):\n",
    "    rates = []\n",
    "    for file_path in file_paths:\n",
    "        sr = librosa.get_samplerate(file_path)\n",
    "        rates.append(sr)\n",
    "    return rates\n",
    "\n",
    "rates = check_sampling_rates(audio_file_paths)\n",
    "unique_rates = set(rates)\n",
    "print(f\"Unique sampling rates in dataset: {unique_rates}\")\n",
    "if len(unique_rates) > 1:\n",
    "    print(\"Warning: Dataset contains multiple sampling rates.\")\n",
    "\n",
    "i = check_sampling_rates(audio_file_paths)\n",
    "print (i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "160fd75b-3c48-4497-a7c2-636f4e7a2e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n",
      "3.6.0\n",
      "WARNING:tensorflow:From C:\\Users\\Peter\\miniconda3\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)\n",
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dc104f2-d547-4a75-9022-3706e5833341",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"functional_23\" is incompatible with the layer: expected shape=(None, 64, 63, 1), found shape=(None, 64, 87)\u001b[0m\n\nArguments received by Sequential.call():\n  • args=('<KerasTensor shape=(None, 64, 87, 1), dtype=float32, sparse=False, name=keras_tensor_163>',)\n  • kwargs={'mask': 'None'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m gan_input \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(latent_dim,))\n\u001b[0;32m     76\u001b[0m generated_image \u001b[38;5;241m=\u001b[39m generator(gan_input)\n\u001b[1;32m---> 77\u001b[0m gan_output \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m gan \u001b[38;5;241m=\u001b[39m Model(gan_input, gan_output)\n\u001b[0;32m     79\u001b[0m gan\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m     80\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     81\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0002\u001b[39m, beta_1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m     82\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    246\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"functional_23\" is incompatible with the layer: expected shape=(None, 64, 63, 1), found shape=(None, 64, 87)\u001b[0m\n\nArguments received by Sequential.call():\n  • args=('<KerasTensor shape=(None, 64, 87, 1), dtype=float32, sparse=False, name=keras_tensor_163>',)\n  • kwargs={'mask': 'None'}"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Conv2D, Conv2DTranspose, Flatten, Reshape,\n",
    "    LeakyReLU, BatchNormalization, ZeroPadding2D, Cropping2D\n",
    ")\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "# Input shape parameters\n",
    "num_samples, N_MELS, time_steps, channels = data.shape\n",
    "input_shape = (N_MELS, time_steps, channels)\n",
    "latent_dim = 100  # Size of the latent space (noise vector)\n",
    "\n",
    "def build_generator(latent_dim):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Starting dimensions\n",
    "    n_rows, n_cols = 8, 11  # Choose values so that upsampling results in (64, 87)\n",
    "    model.add(Dense(256 * n_rows * n_cols, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((n_rows, n_cols, 256)))  # Shape: (8, 11, 256)\n",
    "    \n",
    "    # First upsampling\n",
    "    model.add(Conv2DTranspose(256, kernel_size=4, strides=(2, 2), padding='same'))  # Shape: (16, 22, 256)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    # Second upsampling\n",
    "    model.add(Conv2DTranspose(128, kernel_size=4, strides=(2, 2), padding='same'))  # Shape: (32, 44, 128)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    # Third upsampling\n",
    "    model.add(Conv2DTranspose(64, kernel_size=4, strides=(2, 2), padding='same'))  # Shape: (64, 88, 64)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    # Final adjustment to reach (64, 87, 1)\n",
    "    model.add(Conv2DTranspose(1, kernel_size=4, strides=(1, 1), padding='valid', activation='tanh'))  # Shape: (67, 91, 1)\n",
    "    # Adjusted cropping to achieve the desired output shape\n",
    "    model.add(Cropping2D(cropping=((1, 2), (2, 2))))  # Crop to (64, 87, 1)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_discriminator(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=4, strides=2, padding='same', input_shape=input_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Conv2D(256, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Build the generator and discriminator\n",
    "generator = build_generator(latent_dim)\n",
    "discriminator = build_discriminator(input_shape)\n",
    "\n",
    "# Compile the discriminator\n",
    "discriminator.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5),\n",
    "    run_eagerly=True\n",
    ")\n",
    "\n",
    "# Build and compile the GAN model\n",
    "discriminator.trainable = True  # Freeze the discriminator's weights when training the generator\n",
    "gan_input = Input(shape=(latent_dim,))\n",
    "generated_image = generator(gan_input)\n",
    "gan_output = discriminator(generated_image)\n",
    "gan = Model(gan_input, gan_output)\n",
    "gan.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4627f9ab-232f-4a5f-8037-e765ff49f902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1413, 64, 63, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Conv2D, Conv2DTranspose, Flatten, Reshape,\n",
    "    LeakyReLU, BatchNormalization, ZeroPadding2D, Cropping2D\n",
    ")\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "# Input shape parameters\n",
    "num_samples, N_MELS, time_steps, channels = data.shape\n",
    "print(f\"Input shape: ({num_samples}, {N_MELS}, {time_steps}, {channels})\")\n",
    "input_shape = (N_MELS, time_steps, channels)\n",
    "latent_dim = 100  # Size of the latent space (noise vector)\n",
    "\n",
    "def build_generator(latent_dim):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Starting dimensions - Adjusted for (64, 63) output\n",
    "    n_rows, n_cols = 8, 8  # Choose values so that upsampling results in (64, 64) \n",
    "    model.add(Dense(128 * n_rows * n_cols, input_dim=latent_dim))  \n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((n_rows, n_cols, 128)))  # Shape: (8, 8, 128)\n",
    "    \n",
    "    # First upsampling\n",
    "    model.add(Conv2DTranspose(128, kernel_size=4, strides=(2, 2), padding='same'))  # Shape: (16, 16, 128)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    # Second upsampling\n",
    "    model.add(Conv2DTranspose(64, kernel_size=4, strides=(2, 2), padding='same'))  # Shape: (32, 32, 64)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "\n",
    "    # Third upsampling\n",
    "    model.add(Conv2DTranspose(32, kernel_size=4, strides=(2, 2), padding='same'))  # Shape: (64, 64, 32)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    # Final layer - Adjusted for (64, 63, 1) output\n",
    "    model.add(Conv2DTranspose(1, kernel_size=(3, 3), strides=(1, 1), padding='valid', activation='tanh'))  # Shape: (64, 63, 1)\n",
    "\n",
    "    # Cropping to get the exact desired shape if necessary\n",
    "    model.add(Cropping2D(((0,2), (3,0))))    \n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_discriminator(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=4, strides=2, padding='same', input_shape=input_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Conv2D(256, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Build the generator and discriminator\n",
    "generator = build_generator(latent_dim)\n",
    "discriminator = build_discriminator(input_shape)\n",
    "\n",
    "# Compile the discriminator\n",
    "discriminator.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5),\n",
    "    run_eagerly=True\n",
    ")\n",
    "\n",
    "# Build and compile the GAN model\n",
    "discriminator.trainable = True  # Freeze the discriminator's weights when training the generator\n",
    "gan_input = Input(shape=(latent_dim,))\n",
    "generated_image = generator(gan_input)\n",
    "gan_output = discriminator(generated_image)\n",
    "gan = Model(gan_input, gan_output)\n",
    "gan.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f2c7b8a-26ba-4efc-b9a8-a7495b0db7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator trainable variables: 16\n",
      "Discriminator trainable variables: 12\n",
      "1 [D loss: 1.9609] [G loss: 2.0372]\n",
      "100 [D loss: 0.2051] [G loss: 3.9163]\n",
      "200 [D loss: 0.2145] [G loss: 4.9114]\n",
      "300 [D loss: 0.0683] [G loss: 5.1567]\n",
      "400 [D loss: 0.5063] [G loss: 2.6100]\n",
      "500 [D loss: 0.2208] [G loss: 3.5783]\n",
      "600 [D loss: 0.4821] [G loss: 2.3689]\n",
      "700 [D loss: 0.2634] [G loss: 3.5275]\n",
      "800 [D loss: 0.5644] [G loss: 2.8879]\n",
      "900 [D loss: 0.3330] [G loss: 3.0212]\n",
      "1000 [D loss: 0.5814] [G loss: 3.7845]\n",
      "1100 [D loss: 1.0346] [G loss: 4.8884]\n",
      "1200 [D loss: 0.3142] [G loss: 2.6645]\n",
      "1300 [D loss: 0.4955] [G loss: 2.5401]\n",
      "1400 [D loss: 0.3485] [G loss: 2.4766]\n",
      "1500 [D loss: 0.5230] [G loss: 3.2496]\n",
      "1600 [D loss: 0.9867] [G loss: 2.7117]\n",
      "1700 [D loss: 0.8517] [G loss: 2.1016]\n",
      "1800 [D loss: 0.2692] [G loss: 2.8978]\n",
      "1900 [D loss: 0.7775] [G loss: 0.8287]\n",
      "2000 [D loss: 0.4149] [G loss: 3.3972]\n",
      "2100 [D loss: 0.5642] [G loss: 1.8329]\n",
      "2200 [D loss: 0.3051] [G loss: 2.6673]\n",
      "2300 [D loss: 0.6707] [G loss: 2.7022]\n",
      "2400 [D loss: 0.7430] [G loss: 1.9406]\n",
      "2500 [D loss: 0.3840] [G loss: 2.4103]\n",
      "2600 [D loss: 0.8343] [G loss: 3.8039]\n",
      "2700 [D loss: 0.3988] [G loss: 3.4310]\n",
      "2800 [D loss: 0.3096] [G loss: 2.9484]\n",
      "2900 [D loss: 0.6165] [G loss: 2.5314]\n",
      "3000 [D loss: 0.3742] [G loss: 3.5906]\n",
      "3100 [D loss: 0.5273] [G loss: 2.5634]\n",
      "3200 [D loss: 0.3952] [G loss: 3.4624]\n",
      "3300 [D loss: 1.9653] [G loss: 6.7728]\n",
      "3400 [D loss: 0.3294] [G loss: 3.1066]\n",
      "3500 [D loss: 0.5205] [G loss: 2.2539]\n",
      "3600 [D loss: 0.3252] [G loss: 2.7158]\n",
      "3700 [D loss: 0.5601] [G loss: 2.8862]\n",
      "3800 [D loss: 0.2886] [G loss: 3.3209]\n",
      "3900 [D loss: 0.3060] [G loss: 3.1031]\n",
      "4000 [D loss: 0.3740] [G loss: 2.1870]\n",
      "4100 [D loss: 0.3502] [G loss: 2.4192]\n",
      "4200 [D loss: 0.4403] [G loss: 2.9485]\n",
      "4300 [D loss: 0.5782] [G loss: 2.8086]\n",
      "4400 [D loss: 0.4399] [G loss: 2.4045]\n",
      "4500 [D loss: 0.3901] [G loss: 3.1393]\n",
      "4600 [D loss: 0.4426] [G loss: 3.6688]\n",
      "4700 [D loss: 0.6690] [G loss: 3.9767]\n",
      "4800 [D loss: 0.2934] [G loss: 2.7405]\n",
      "4900 [D loss: 0.5979] [G loss: 4.8807]\n",
      "5000 [D loss: 0.3008] [G loss: 2.1776]\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "epochs = 5000\n",
    "batch_size = 32\n",
    "save_interval = 1000  # Save generated samples every 1000 epochs\n",
    "\n",
    "# Labels for real and fake images (unused in custom loop but kept for completeness)\n",
    "real = np.ones((batch_size, 1), dtype=np.float32)\n",
    "fake = np.zeros((batch_size, 1), dtype=np.float32)\n",
    "\n",
    "# Define loss function\n",
    "binary_cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "# Define optimizers\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "# Build the generator and discriminator (ensure these functions are defined)\n",
    "generator = build_generator(latent_dim)\n",
    "discriminator = build_discriminator(input_shape)\n",
    "\n",
    "# Ensure models are trainable\n",
    "generator.trainable = True\n",
    "discriminator.trainable = True\n",
    "\n",
    "# Training step function\n",
    "@tf.function\n",
    "def train_step(real_images):\n",
    "    # Generate noise\n",
    "    noise = tf.random.normal([batch_size, latent_dim])\n",
    "\n",
    "    # Train the discriminator\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        # Generate fake images\n",
    "        generated_images = generator(noise, training=True)\n",
    "        \n",
    "        # Discriminator outputs\n",
    "        real_output = discriminator(real_images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        # Calculate discriminator loss\n",
    "        disc_loss_real = binary_cross_entropy(tf.ones_like(real_output), real_output)\n",
    "        disc_loss_fake = binary_cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "        disc_loss = disc_loss_real + disc_loss_fake\n",
    "\n",
    "    # Calculate discriminator gradients\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    # Apply discriminator gradients\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    # Train the generator\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        # Generate fake images\n",
    "        generated_images = generator(noise, training=True)\n",
    "        \n",
    "        # Discriminator output for generated images\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        # Calculate generator loss\n",
    "        gen_loss = binary_cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "    # Calculate generator gradients\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    \n",
    "    # Apply generator gradients\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "    return disc_loss, gen_loss\n",
    "\n",
    "# Check trainable variables\n",
    "print(f\"Generator trainable variables: {len(generator.trainable_variables)}\")\n",
    "print(f\"Discriminator trainable variables: {len(discriminator.trainable_variables)}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Select a random batch of real images\n",
    "    idx = np.random.randint(0, data.shape[0], batch_size)\n",
    "    real_imgs = data[idx]\n",
    "\n",
    "    # Perform a training step\n",
    "    d_loss, g_loss = train_step(real_imgs)\n",
    "\n",
    "    # Print progress\n",
    "    if epoch % 100 == 0 or epoch == 1:\n",
    "        print(f\"{epoch} [D loss: {d_loss.numpy():.4f}] [G loss: {g_loss.numpy():.4f}]\")\n",
    "\n",
    "    # If at save interval, save generated image samples\n",
    "    if epoch % save_interval == 0:\n",
    "        # Generate and save images\n",
    "        noise = tf.random.normal([1, latent_dim])\n",
    "        gen_img = generator(noise, training=False)\n",
    "        gen_img = gen_img.numpy().squeeze()\n",
    "\n",
    "        # Rescale back to original scale\n",
    "        gen_img = (gen_img + 1) / 2  # Scale from [-1, 1] to [0, 1]\n",
    "        gen_img = gen_img * (data_max - data_min) + data_min  # Rescale to original data range\n",
    "\n",
    "        # Save the generated image\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        librosa.display.specshow(gen_img, sr=SR, hop_length=HOP_LENGTH, x_axis='time', y_axis='mel')\n",
    "        plt.title(f\"Generated Mel Spectrogram at Epoch {epoch}\")\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"generated_image_epoch_{epoch}.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30a23502-dc31-43aa-8e14-915695561208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n",
      "Generated audio saved to generated_audio_I.wav\n"
     ]
    }
   ],
   "source": [
    "def generate_audio_from_mel(generator, latent_dim, filename):\n",
    "    # Generate mel-spectrogram\n",
    "    noise = np.random.normal(0, 1, (1, latent_dim))\n",
    "    gen_mel = generator.predict(noise)\n",
    "    gen_mel = gen_mel.squeeze()\n",
    "    \n",
    "    # Rescale the generated mel-spectrogram\n",
    "    gen_mel = (gen_mel + 1) / 2  # Scale from [-1, 1] to [0, 1]\n",
    "    gen_mel = gen_mel * (data_max - data_min) + data_min  # Rescale to original data range\n",
    "    \n",
    "    # Convert mel-spectrogram (in dB) to power\n",
    "    gen_mel = librosa.db_to_power(gen_mel)\n",
    "    \n",
    "    # Invert the mel-spectrogram to a waveform\n",
    "    y = librosa.feature.inverse.mel_to_audio(\n",
    "        gen_mel,\n",
    "        sr=SR,\n",
    "        n_fft=2048,\n",
    "        hop_length=HOP_LENGTH,\n",
    "        win_length=2048,\n",
    "        n_iter=60,\n",
    "        power=1.0\n",
    "    )\n",
    "    \n",
    "    # Save the audio using soundfilehttps://www.tensorflow.org/api_docs/python/tf/function\n",
    "    sf.write(filename, y, SR)\n",
    "    print(f\"Generated audio saved to {filename}\")\n",
    "    \n",
    "# Example usage\n",
    "generate_audio_from_mel(generator, latent_dim, 'generated_audio_I.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53e463f5-3df4-42a9-a31b-e410cfa2462d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save final models\n",
    "generator.save(\"final_generator_I.h5\")\n",
    "discriminator.save(\"final_discriminator_I.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f3ebaa6-b7ce-4006-87db-09766228b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final models\n",
    "generator.save(\"final_generator_I.keras\")\n",
    "discriminator.save(\"final_discriminator_I.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a8b6b94-afe0-42af-88e1-cbecea5c2a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_0.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_1.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_2.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_3.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_4.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_5.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_6.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_7.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_8.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_9.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_10.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_11.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_12.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_13.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_14.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_15.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_16.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_17.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_18.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_19.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_20.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_21.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_22.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_23.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_24.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_25.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_26.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_27.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_28.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_29.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_30.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_31.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_32.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_33.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_34.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_35.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_36.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_37.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_38.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_39.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_40.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_41.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_42.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_43.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_44.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_45.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_46.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_47.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_48.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_49.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_50.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_51.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_52.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_53.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_54.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_55.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_56.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_57.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_58.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_59.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_60.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_61.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_62.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_63.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_64.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_65.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_66.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_67.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_68.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_69.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_70.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_71.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_72.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_73.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_74.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_75.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_76.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_77.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_78.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_79.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_80.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_81.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_82.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_83.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_84.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_85.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_86.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_87.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_88.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_89.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_90.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_91.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_92.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_93.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_94.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_95.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_96.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_97.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_98.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Generated audio saved to augmented_data_fixed_Is\\synthetic_sample_99.wav\n"
     ]
    }
   ],
   "source": [
    "def generate_audio_from_mel_fixed(generator, latent_dim, filename, max_duration=20.0):\n",
    "    \"\"\"\n",
    "    Generate audio from a trained GAN model and save it with fixed settings.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import librosa\n",
    "    import soundfile as sf\n",
    "\n",
    "    # Generate mel-spectrogram\n",
    "    noise = np.random.normal(0, 1, (1, latent_dim))\n",
    "    gen_mel = generator.predict(noise)\n",
    "    gen_mel = gen_mel.squeeze()\n",
    "\n",
    "    # Rescale the generated mel-spectrogram\n",
    "    gen_mel = (gen_mel + 1) / 2  # Scale from [-1, 1] to [0, 1]\n",
    "    gen_mel = gen_mel * (data_max - data_min) + data_min  # Rescale to original data range\n",
    "\n",
    "    # Convert mel-spectrogram (in dB) to power\n",
    "    gen_mel = librosa.db_to_power(gen_mel)\n",
    "\n",
    "    # Invert the mel-spectrogram to a waveform\n",
    "    y = librosa.feature.inverse.mel_to_audio(\n",
    "        gen_mel,\n",
    "        sr=SR,\n",
    "        n_fft=2048,\n",
    "        hop_length=HOP_LENGTH,\n",
    "        win_length=2048,\n",
    "        n_iter=60,\n",
    "        power=1.0\n",
    "    )\n",
    "\n",
    "    # Adjust length to be slightly variable but within 20 seconds\n",
    "    max_samples = int(SR * max_duration)\n",
    "    y = librosa.util.fix_length(y, size=np.random.randint(max_samples - 1000, max_samples + 1000))\n",
    "\n",
    "    # Save the audio using soundfile\n",
    "    sf.write(filename, y, SR)\n",
    "    print(f\"Generated audio saved to {filename}\")\n",
    "\n",
    "def generate_batch_audio_fixed(generator, latent_dim, num_samples, folder_path, max_duration=4.0):\n",
    "    \"\"\"\n",
    "    Generate a batch of audio files and save them to a folder.\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    for i in range(num_samples):\n",
    "        filename = os.path.join(folder_path, f\"synthetic_sample_{i}.wav\")\n",
    "        generate_audio_from_mel_fixed(generator, latent_dim, filename, max_duration=max_duration)\n",
    "\n",
    "generate_batch_audio_fixed(generator, latent_dim, num_samples=100, folder_path=\"augmented_data_fixed_Is\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ffb22c-554e-4e35-bad2-da462380f4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
